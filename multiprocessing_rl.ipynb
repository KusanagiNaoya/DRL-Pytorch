{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KusanagiNaoya/DRL-Pytorch/blob/main/multiprocessing_rl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyyN-2qyK_T2"
      },
      "source": [
        "# Stable Baselines3 - Easy Multiprocessing\n",
        "\n",
        "Github Repo: [https://github.com/DLR-RM/stable-baselines3](https://github.com/DLR-RM/stable-baselines3)\n",
        "\n",
        "\n",
        "[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) is a training framework for Reinforcement Learning (RL), using Stable Baselines3.\n",
        "\n",
        "It provides scripts for training, evaluating agents, tuning hyperparameters, plotting results and recording videos.\n",
        "\n",
        "Documentation is available online: [https://stable-baselines3.readthedocs.io/](https://stable-baselines3.readthedocs.io/)\n",
        "\n",
        "## Install Dependencies and Stable Baselines Using Pip\n",
        "\n",
        "\n",
        "```\n",
        "pip install stable-baselines3[extra]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pjcmcy2EIoc"
      },
      "outputs": [],
      "source": [
        "# for autoformatting\n",
        "# %load_ext jupyter_black"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "503Gi2076F7u"
      },
      "outputs": [],
      "source": [
        "!pip install \"stable-baselines3[extra]>=2.0.0a4\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtY8FhliLsGm"
      },
      "source": [
        "## Import policy, RL agent, ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIedd7Pz9sOs"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "from stable_baselines3 import A2C\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
        "from stable_baselines3.common.utils import set_random_seed\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.env_util import make_vec_env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5WNF6G5gWZ1"
      },
      "source": [
        "## Multiprocessing RL Training\n",
        "\n",
        "To multiprocess RL training, we will just have to wrap the Gym env into a `SubprocVecEnv` object, that will take care of synchronising the processes. The idea is that each process will run an indepedent instance of the Gym env.\n",
        "\n",
        "For that, we need an additional utility function, `make_env`, that will instantiate the environments and make sure they are different (using different random seed)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgjfyOTPVxG6"
      },
      "outputs": [],
      "source": [
        "from typing import Callable\n",
        "\n",
        "\n",
        "def make_env(env_id: str, rank: int, seed: int = 0) -> Callable:\n",
        "    \"\"\"\n",
        "    Utility function for multiprocessed env.\n",
        "\n",
        "    :param env_id: (str) the environment ID\n",
        "    :param num_env: (int) the number of environment you wish to have in subprocesses\n",
        "    :param seed: (int) the inital seed for RNG\n",
        "    :param rank: (int) index of the subprocess\n",
        "    :return: (Callable)\n",
        "    \"\"\"\n",
        "\n",
        "    def _init() -> gym.Env:\n",
        "        env = gym.make(env_id)\n",
        "        env.reset(seed=seed + rank)\n",
        "        return env\n",
        "\n",
        "    set_random_seed(seed)\n",
        "    return _init"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPGIySi2g_RN"
      },
      "source": [
        "The number of parallel process used is defined by the `num_cpu` variable.\n",
        "\n",
        "Because we use vectorized environment (SubprocVecEnv), the actions sent to the wrapped env must be an array (one action per process). Also, observations, rewards and dones are arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUWGZp3i9wyf"
      },
      "outputs": [],
      "source": [
        "env_id = \"CartPole-v1\"\n",
        "num_cpu = 4  # Number of processes to use\n",
        "# Create the vectorized environment\n",
        "env = SubprocVecEnv([make_env(env_id, i) for i in range(num_cpu)])\n",
        "\n",
        "model = A2C(\"MlpPolicy\", env, verbose=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPtIY0dR6ssd"
      },
      "source": [
        "Stable-Baselines3 provides you with make_vec_env() helper which does exactly the previous steps for you:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TYDgQHz6sIV"
      },
      "outputs": [],
      "source": [
        "# By default, we use a DummyVecEnv as it is usually faster (cf doc)\n",
        "vec_env = make_vec_env(env_id, n_envs=num_cpu)\n",
        "\n",
        "model = A2C(\"MlpPolicy\", vec_env, verbose=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjEVOIY8NVeK"
      },
      "source": [
        "Let's evaluate the un-trained agent, this should be a random agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDHLMA6NFk95"
      },
      "outputs": [],
      "source": [
        "# We create a separate environment for evaluation\n",
        "eval_env = gym.make(env_id)\n",
        "\n",
        "# Random Agent, before training\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10)\n",
        "print(f\"Mean reward: {mean_reward} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5UoXTZPNdFE"
      },
      "source": [
        "## Multiprocess VS Single Process Training\n",
        "\n",
        "Here, we will compare time taken using one vs 4 processes, it should take ~30s in total."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4cfSXIB-pTF"
      },
      "outputs": [],
      "source": [
        "n_timesteps = 25000\n",
        "\n",
        "# Multiprocessed RL Training\n",
        "start_time = time.time()\n",
        "model.learn(n_timesteps)\n",
        "total_time_multi = time.time() - start_time\n",
        "\n",
        "print(\n",
        "    f\"Took {total_time_multi:.2f}s for multiprocessed version - {n_timesteps / total_time_multi:.2f} FPS\"\n",
        ")\n",
        "\n",
        "# Single Process RL Training\n",
        "single_process_model = A2C(\"MlpPolicy\", env_id, verbose=0)\n",
        "\n",
        "start_time = time.time()\n",
        "single_process_model.learn(n_timesteps)\n",
        "total_time_single = time.time() - start_time\n",
        "\n",
        "print(\n",
        "    f\"Took {total_time_single:.2f}s for single process version - {n_timesteps / total_time_single:.2f} FPS\"\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"Multiprocessed training is {:.2f}x faster!\".format(\n",
        "        total_time_single / total_time_multi\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygl_gVmV_QP7"
      },
      "outputs": [],
      "source": [
        "# Evaluate the trained agent\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10)\n",
        "print(f\"Mean reward: {mean_reward} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkWsoZ8emt0e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "multiprocessing_rl.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}